### **联合国安理会会议记录爬取过程中遇到的问题及策略分析**

在开发和优化联合国安理会会议记录爬取系统的过程中，我们遇到了多种技术挑战，包括网络请求处理、HTML 解析、文件管理、调试优化等。此外，考虑到目标网站的反爬虫机制以及数据动态变化的特性，我们也需要设计合理的爬取策略来确保数据获取的稳定性、完整性和可持续性。以下是整个爬取过程中的问题及相应的策略分析。

---

## **1. 网络请求相关问题**
### **1.1 反爬虫机制**
- **问题**：联合国数字图书馆（https://digitallibrary.un.org）对高频请求具有一定的反爬虫机制，例如：
  - 过多的短时间请求可能触发 **429 Too Many Requests**。
  - 某些 IP 可能会被暂时封锁，导致请求返回 **403 Forbidden**。
  - 服务器可能返回 **503 Service Unavailable**，防止高频访问。
- **策略**：
  - **随机时间间隔**：在每次请求之间添加 `random.uniform(1, 3)` 秒的随机延迟，防止高频访问触发封锁。
  - **重试机制**：使用 `urllib3.util.retry.Retry` 机制，对于 `429, 500, 502, 503, 504` 等错误进行自动重试，减少因临时网络波动导致的失败。
  - **动态 User-Agent**：可以考虑在不同请求中轮换 `User-Agent` 头部，模拟不同浏览器，降低被识别为爬虫的风险（当前代码中暂未实现）。
  - **代理 IP 轮换**：可引入动态 IP 代理池，如 `Tor` 网络、付费代理或开源代理池（如 `scrapy-rotating-proxies`），实现 IP 轮换。

### **1.2 请求失败与网络异常**
- **问题**：
  - 网络波动可能导致请求超时或失败。
  - 目标服务器可能短暂不可用，返回 5xx 错误。
- **策略**：
  - **超时设置**：为 `requests.get()` 添加 `timeout=15`，避免长时间卡死。
  - **自动重试**：使用 `requests.adapters.HTTPAdapter` 配合 `Retry`，对可恢复的错误（如 5xx、429）进行重试。

---

## **2. 网站结构解析问题**
### **2.1 会议详情页链接的提取**
- **问题**：
  - 会议详情页的链接最初被误认为是 `result-title` 类的 `<div>` 元素，而实际上它隐藏在 `moreinfo` 类下的 `<a>` 标签。
  - `href` 链接是相对路径（`/record/xxxx`），必须拼接 `BASE_URL` 形成完整链接。
- **策略**：
  - 调整解析逻辑，提取 `div.moreinfo > a.moreinfo` 作为会议详情页链接。
  - 解析出相对路径后，使用 `BASE_URL + href` 组合完整 URL。

### **2.2 会议记录页面的提取**
- **问题**：
  - “Meeting Record” 链接位置并不统一，部分页面中它位于 `<div class="metadata-row">`，需要先匹配 `span.title`，然后从 `span.value` 内提取 `<a>` 链接。
- **策略**：
  - 采用 HTML 结构分析法，使用 `soup.find_all("div", class_="metadata-row")` 先定位 `Meeting record`，再提取对应的 `<a>` 链接。

### **2.3 会议记录 PDF 文件的获取**
- **问题**：
  - PDF 文件下载链接并非标准 `<a>` 标签，而是嵌套在 `<tindui-app-file-download-link>` 组件中。
  - 仅部分会议提供英文 PDF，有些会议仅有其他语言版本。
- **策略**：
  - 先查找 `<a>` 标签中的 `-EN.pdf` 关键词，优先匹配英文 PDF。
  - 若找不到 `<a>`，则尝试提取 `tindui-app-file-download-link` 组件中的 `url`。

---

## **3. 文件管理与调试**
### **3.1 下载文件覆盖问题**
- **问题**：
  - 爬虫在重复运行时，可能会重复下载已存在的 PDF 文件，造成资源浪费。
- **策略**：
  - 先检查 `os.path.exists(filepath)`，若文件已存在，则跳过下载。

### **3.2 调试 HTML 结构**
- **问题**：
  - 由于目标网站的 HTML 结构较复杂，在初期调试时难以定位具体数据元素。
  - 部分会议页面的 HTML 结构存在较大差异，导致部分页面解析失败。
- **策略**：
  - 在 `DEBUG_MODE` 下，保存每个页面的完整 HTML，以便人工分析。
  - 限制 HTML 预览内容至前 `1000` 字符，提高可读性。
  - 添加 `print(f"页面标题: {soup.title.text}")`，确保页面加载正确。

### **3.3 解决 Windows 终端编码问题**
- **问题**：
  - Windows 终端默认使用 `GBK` 编码，导致输出 Unicode 表情符号时报错。
- **策略**：
  - 使用 `sys.stdout.reconfigure(encoding='utf-8')` 重新配置终端编码。

---

## **4. 爬取策略优化**
### **4.1 分页爬取的局限性**
- **问题**：
  - 目标网站的会议记录会不断更新，静态分页爬取方式无法持续跟踪新增数据。
  - 直接按 `jrec` 参数翻页可能导致部分记录遗漏或重复。
- **策略**：
  - **动态增量爬取**：
    - 记录上次爬取的最新会议编号，下次运行时只爬取新增数据，避免重复爬取已下载的会议。
  - **基于时间范围爬取**：
    - 可以设计按日期筛选的机制，只爬取最近 X 天的数据，而不是按固定页数。

### **4.2 用户自定义爬取范围**
- **问题**：
  - 早期代码只能爬取固定的 `max_pages=1`，无法灵活选择起始页和终止页。
- **策略**：
  - 允许用户输入 `start_page` 和 `end_page`，指定爬取范围。
  - 采用 `range(start_page - 1, end_page)` 逻辑，确保页面索引与用户输入一致。

---

## **5. 运行环境与执行问题**
### **5.1 VS Code 终端只读问题**
- **问题**：
  - 在 VS Code 内运行 Python 脚本时，终端无法输入参数，导致程序无法交互运行。
- **策略**：
  - 解决方案 1：在 `cmd` 或 `PowerShell` 终端运行脚本，避免 VS Code 限制。
  - 解决方案 2：修改代码支持从 `.json` 配置文件或命令行参数读取用户输入。

### **5.2 生成 `requirements.txt`**
- **问题**：
  - 需要导出依赖环境，但不清楚具体命令。
- **策略**：
  - 运行 `pip freeze > requirements.txt` 生成依赖文件。

---

## **结论**
整个爬虫开发过程中，我们不仅解决了网站解析和反爬虫问题，还针对数据存储、调试优化、运行环境等方面进行了调整。同时，我们考虑了后续可能遇到的挑战，例如动态增量爬取、代理 IP 轮换等，未来可以继续优化爬虫的效率和稳定性。

目前，代码已经能够稳定爬取会议记录，但仍有以下优化空间：
- **增量爬取**：避免重复下载已爬取的数据。
- **动态代理**：提高爬取的隐蔽性，绕过 IP 限制。
- **数据存储管理**：爬取数据持久化至数据库，提高可查询性。

未来可基于这些改进点进一步优化爬虫架构，使其更加智能化和可扩展。🚀